{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNrkDoqTXKrFsJ2ur/oI5B2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Propa-Punam/Wifi-RSS-Crowdsensing/blob/main/temporary/told_to_be_exactly_like_boook_but_with_0_accuracy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rVX0vGxtnoM-",
        "outputId": "d67693ea-764a-45a2-e6df-c39d979e585c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.00%\n",
            "\n",
            "Confusion Matrix:\n",
            "Zone 203: 0/12 (0.00%)\n",
            "Zone 204: 0/11 (0.00%)\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "class CounterPropagationNetwork:\n",
        "    def __init__(self, input_dim, num_zones):\n",
        "        self.input_dim = input_dim  # n in the algorithm (number of APs)\n",
        "        self.num_zones = num_zones  # m in the algorithm (number of clusters/zones)\n",
        "        self.num_bits = int(np.ceil(np.log2(num_zones)))\n",
        "\n",
        "        # Initialize weights for Kohonen layer (wij in the algorithm)\n",
        "        self.kohonen_weights = np.zeros((num_zones, input_dim))\n",
        "\n",
        "        # Initialize weights for Grossberg layer\n",
        "        self.grossberg_weights = np.zeros((num_zones, self.num_bits))\n",
        "\n",
        "        # Gray code mapping for zones\n",
        "        self.gray_codes = {\n",
        "            '203': [0, 0, 0],\n",
        "            '204': [0, 0, 1],\n",
        "            'l1': [0, 1, 1],\n",
        "            'l2': [0, 1, 0],\n",
        "            'l3': [1, 1, 0]\n",
        "        }\n",
        "\n",
        "        self.zone_mapping = {\n",
        "            0: '203',\n",
        "            1: '204',\n",
        "            2: 'l1',\n",
        "            3: 'l2',\n",
        "            4: 'l3'\n",
        "        }\n",
        "\n",
        "    def preprocess_data(self, data):\n",
        "        signal_columns = ['CSE-104', 'DataLab@BUET', 'Galaxy M124213', 'Hall of Fame',\n",
        "                         'CSE-206', 'CSE-303', 'CSE-205', 'CSE-204', 'CSE-304', 'CSE-202']\n",
        "\n",
        "        # Replace -100 with -130 as mentioned in the paper\n",
        "        data[signal_columns] = data[signal_columns].replace(-100, -130)\n",
        "\n",
        "        # Normalize RSSI values\n",
        "        data[signal_columns] = (data[signal_columns] + 130) / 90\n",
        "\n",
        "        return data[signal_columns].values\n",
        "\n",
        "    def initialize_weights(self, training_data, rooms):\n",
        "        # Step 1: Initialize weight vectors with sample mean vectors for each location\n",
        "        for zone_idx, zone in enumerate(np.unique(rooms)):\n",
        "            zone_data = training_data[rooms == zone]\n",
        "            self.kohonen_weights[zone_idx] = np.mean(zone_data, axis=0)\n",
        "            self.grossberg_weights[zone_idx] = self.gray_codes[zone]\n",
        "\n",
        "    def train_kohonen(self, training_data, rooms, learning_rate=0.1, max_iterations=1000):\n",
        "        # Following exactly Algorithm 3 from the paper\n",
        "\n",
        "        # Step 1: Initialize weights with mean vectors\n",
        "        self.initialize_weights(training_data, rooms)\n",
        "\n",
        "        # Step 2: repeat\n",
        "        for iteration in range(max_iterations):\n",
        "            # Step 3: for each vector Vz,t = (rss1, rss2, ..., rssn)\n",
        "            for idx, input_vector in enumerate(training_data):\n",
        "                val_j = np.zeros(self.num_zones)\n",
        "\n",
        "                # Step 4-5: for j = 1 to m do calculate VALj = Σi rssi × wij\n",
        "                for j in range(self.num_zones):\n",
        "                    val_j[j] = np.sum(input_vector * self.kohonen_weights[j])\n",
        "\n",
        "                # Step 8: Assign input vector to cluster Lj for which VALj is maximum\n",
        "                winner_idx = np.argmax(val_j)\n",
        "\n",
        "                # Step 9-10: for i = 1 to n do wij(new) = wij(old) + β(rssj - wij(old))\n",
        "                for i in range(self.input_dim):\n",
        "                    old_weight = self.kohonen_weights[winner_idx][i]\n",
        "                    self.kohonen_weights[winner_idx][i] = old_weight + learning_rate * (input_vector[i] - old_weight)\n",
        "\n",
        "            # Decrease learning rate\n",
        "\n",
        "\n",
        "    def predict(self, test_data):\n",
        "        predictions = []\n",
        "\n",
        "        for input_vector in test_data:\n",
        "            # Calculate VALj for each cluster\n",
        "            val_j = np.zeros(self.num_zones)\n",
        "            for j in range(self.num_zones):\n",
        "                val_j[j] = np.sum(input_vector * self.kohonen_weights[j])\n",
        "\n",
        "            # Find winning neuron\n",
        "            winner_idx = np.argmax(val_j)\n",
        "            predicted_zone = self.zone_mapping[winner_idx]\n",
        "            predictions.append(predicted_zone)\n",
        "\n",
        "        return predictions\n",
        "\n",
        "def evaluate_model(y_true, y_pred):\n",
        "    correct = sum(1 for true, pred in zip(y_true, y_pred) if true == pred)\n",
        "    accuracy = correct / len(y_true)\n",
        "\n",
        "    print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
        "    print(\"\\nConfusion Matrix:\")\n",
        "    for true_zone in sorted(set(y_true)):\n",
        "        correct = sum(1 for true, pred in zip(y_true, y_pred)\n",
        "                     if true == true_zone and true == pred)\n",
        "        total = sum(1 for true in y_true if true == true_zone)\n",
        "        print(f\"Zone {true_zone}: {correct}/{total} ({correct/total*100:.2f}%)\")\n",
        "\n",
        "def main():\n",
        "    # Load data\n",
        "    train_df = pd.read_csv('/content/train.csv')\n",
        "    test_df = pd.read_csv('/content/test.csv')\n",
        "\n",
        "    # Initialize model\n",
        "    cpn = CounterPropagationNetwork(input_dim=10, num_zones=5)\n",
        "\n",
        "    # Preprocess data\n",
        "    X_train = cpn.preprocess_data(train_df)\n",
        "    y_train = train_df['room'].values\n",
        "\n",
        "    # Train using Algorithm 3\n",
        "    cpn.train_kohonen(X_train, y_train)\n",
        "\n",
        "    # Test\n",
        "    X_test = cpn.preprocess_data(test_df)\n",
        "    y_test = test_df['room'].values\n",
        "    predictions = cpn.predict(X_test)\n",
        "\n",
        "    # Evaluate\n",
        "    evaluate_model(y_test, predictions)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    }
  ]
}